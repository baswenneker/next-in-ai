{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOePYbg5gl8BULFGjJcJuwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baswenneker/next-in-ai/blob/master/next_in_ai_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize Pocket articles and save to docx.\n",
        "For more info, go to the [GitHub](https://github.com/baswenneker/next-in-ai) repo."
      ],
      "metadata": {
        "id": "CMr6GZrUmJOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 1: Configuration. Add keys and urls.\n",
        "\n",
        "# Get it here: https://help.getpocket.com/article/1074-can-i-subscribe-to-my-list-via-rss\n",
        "os.environ['POCKET_RSS_FEED'] = \"https://getpocket.com/users/*sso1456990609615e33/feed/all\"\n",
        "if os.environ['POCKET_RSS_FEED'] == \"https://getpocket.com/users/<YOURPOCKETUSERNAME>/feed/all\":\n",
        "  raise Exception(\"❌ You forgot to add the url to your Pocket RSS feed.\")\n",
        "\n",
        "# Enter your API key (find it here https://platform.openai.com/account/api-keys).\n",
        "os.environ['OPENAI_API_KEY'] = getpass('Enter your paid OpenAI API key here (find it here https://platform.openai.com/account/api-keys):')\n",
        "\n",
        "# Beehiiv newsletter page (used to determine which Pocket articles are fetched).\n",
        "# Leave empty to get all articles from the last 7 days.\n",
        "# For example: \"https://nextinai.beehiiv.com/\"\n",
        "os.environ['BEEHIIV_URL']=''\n",
        "\n",
        "# Use \"gpt-3.5-turbo\" (default) or \"gpt-4\". GPT-4 gives the best results, but is more expensive.\n",
        "os.environ['MODEL'] = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Leave empty for English:\n",
        "os.environ['OUTPUT_LANGUAGE'] =\"Dutch\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8WoVAUAk-y0",
        "outputId": "57c9e1ba-fe3c-43b2-fa07-bdf2c568672f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your paid OpenAI API key here (find it here https://platform.openai.com/account/api-keys):··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 2: Set up and install requirements.\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "!rm -rf /content/next-in-ai/\n",
        "!git clone https://github.com/baswenneker/next-in-ai.git\n",
        "\n",
        "!pip install -r /content/next-in-ai/.devcontainer/requirements.txt\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, \"/content/next-in-ai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAmEALajYmAO",
        "outputId": "6a2cb91d-1919-4bc1-9523-3eb10e5f6cb9",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Cloning into 'next-in-ai'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 107 (delta 59), reused 86 (delta 40), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (107/107), 81.09 KiB | 1.29 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: readtime in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 5)) (0.8.11)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (0.27.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 8)) (4.11.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 9)) (2022.7.1)\n",
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.10/dist-packages (from -r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (1.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r /content/next-in-ai/.devcontainer/requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->-r /content/next-in-ai/.devcontainer/requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: pyquery>=1.2 in /usr/local/lib/python3.10/dist-packages (from readtime->-r /content/next-in-ai/.devcontainer/requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: markdown2>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from readtime->-r /content/next-in-ai/.devcontainer/requirements.txt (line 4)) (2.4.8)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx->-r /content/next-in-ai/.devcontainer/requirements.txt (line 5)) (4.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (3.8.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->-r /content/next-in-ai/.devcontainer/requirements.txt (line 8)) (2.4.1)\n",
            "Requirement already satisfied: courlan>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (0.9.2)\n",
            "Requirement already satisfied: htmldate>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (1.2.3)\n",
            "Requirement already satisfied: justext>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (3.0.0)\n",
            "Requirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from courlan>=0.7.2->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (3.3.0)\n",
            "Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.10/dist-packages (from courlan>=0.7.2->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (0.13)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from htmldate>=1.2.1->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: dateparser>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from htmldate>=1.2.1->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (1.1.8)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyquery>=1.2->readtime->-r /content/next-in-ai/.devcontainer/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (1.9.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r /content/next-in-ai/.devcontainer/requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.1->htmldate>=1.2.1->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (4.3)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.1->htmldate>=1.2.1->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (2022.10.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->htmldate>=1.2.1->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (1.16.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser>=1.1.1->htmldate>=1.2.1->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser>=1.1.1->htmldate>=1.2.1->trafilatura->-r /content/next-in-ai/.devcontainer/requirements.txt (line 10)) (2023.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to fetch the urls from Pocket."
      ],
      "metadata": {
        "id": "qiJ8WCsh-k6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3: Fetch new Pocket article urls.\n",
        "\n",
        "%autoreload\n",
        "from next_in_ai.PocketParser import PocketParser\n",
        "\n",
        "p = PocketParser()\n",
        "\n",
        "# Get all Pocket urls since you sent out the last newsletter (works with Beehiiv):\n",
        "# latest_pocket_urls = p.new_articles()\n",
        "# Get all Pocket urls added since 7 days:\n",
        "latest_pocket_urls = p.new_articles_from_days_ago(2)\n",
        "\n",
        "print(\"We got the following URLs from Pocket:\")\n",
        "\n",
        "for url in latest_pocket_urls:\n",
        "  print(f\" - {url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-1eaIbeaY67",
        "outputId": "0b8de176-38bd-4b0a-ae3e-0645b449af0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We got the following URLs from Pocket:\n",
            " - https://huggingface.co/blog/starcoder?utm_source=pocket_saves\n",
            " - https://semianalysis.com/p/google-we-have-no-moat-and-neither?r=8ryw\n",
            " - https://kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, try to fetch all the content from the urls and summarize it with GPT.\n",
        "\n",
        "⚠️ Disclaimer: some websites might be unavailable or block our crawler on purpose."
      ],
      "metadata": {
        "id": "hY1X6nHPA-0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4: Scrape the articles and summarize using OpenAI.\n",
        "\n",
        "%autoreload\n",
        "from next_in_ai.BatchSummarizer import BatchSummarizer\n",
        "\n",
        "batch_summarizer = BatchSummarizer(latest_pocket_urls)\n",
        "batch_summarizer.create_summary_document(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoBIWSZ0-VJc",
        "outputId": "516469c6-cd90-418d-f1a9-d197b218c233",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:trafilatura.downloads:retries/redirects: https://semianalysis.com/p/google-we-have-no-moat-and-neither?r=8ryw HTTPSConnectionPool(host='semianalysis.com', port=443): Max retries exceeded with url: /p/google-we-have-no-moat-and-neither?r=8ryw (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f42366c3f40>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n",
            "ERROR:trafilatura.utils:lxml parsing failed: Document is empty\n",
            "ERROR:trafilatura.core:empty HTML tree for URL None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Creating the summary document\n",
            "\n",
            "\n",
            "Summarizing article:  https://huggingface.co/blog/starcoder?utm_source=pocket_saves\n",
            "🤗 Got summary from cache.\n",
            "\n",
            "\n",
            "Summarizing article:  https://semianalysis.com/p/google-we-have-no-moat-and-neither?r=8ryw\n",
            "🌎 Getting the contents of the url.\n",
            "<next_in_ai.OpenAISummarizer.OpenAISummarizer object at 0x7f42366c3760> None\n",
            "❌ Couldn't fetch content from https://semianalysis.com/p/google-we-have-no-moat-and-neither?r=8ryw\n",
            "\n",
            "\n",
            "Summarizing article:  https://kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html\n",
            "🌎 Getting the contents of the url.\n",
            "<next_in_ai.OpenAISummarizer.OpenAISummarizer object at 0x7f42366c37f0> HuggingGPT: The Secret Weapon to Solve Complex AI Tasks\n",
            "Get ready to discover the next big thing in AI with HuggingGPT. Read this article to develop an understanding of how it works and how it handles complex AI tasks.\n",
            "Image by Author\n",
            "Introduction\n",
            "Have you heard of the term Artificial General Intelligence (AGI)? If not, let me clarify. AGI can be thought of as an AI system that can understand, process, and respond the intellectual tasks just like humans do. It's a challenging task that requires an in-depth understanding of how the human brain works so we can replicate it. However, the advent of ChatGPT has drawn immense interest from the research community to develop such systems. Microsoft has released one such key AI-powered system called HuggingGPT (Microsoft Jarvis). It is one of the most mind-blowing things that I have come across.\n",
            "Before I dive into the details of what is new in HuggingGPT and how it works, let us first understand the issue with ChatGPT and why it struggles to solve complex AI tasks. Large Language models like ChatGPT excel at interpreting textual data and handling general tasks. However, they often struggle with specific tasks and may generate absurd responses. You might have encountered bogus replies from ChatGPT while solving complex mathematical problems. On the other side, we have expert AI models like Stable Diffusion, and DALL-E that have a deeper understanding of their subject area but struggle with the broader tasks. We cannot fully harness the potential of LLMs to solve challenging AI tasks unless we develop a connection between them and the Specialized AI models. This is what HuggingGPT did. It combined the strengths of both to create more efficient, accurate, and versatile AI systems.\n",
            "What is HuggingGPT?\n",
            "According to a recent paper published by Microsoft, HuggingGPT leverages the power of LLMs by using it as a controller to connect them to various AI models in Machine Learning communities (HuggingFace). Rather than training the ChatGPT for various tasks, we enable it to use external tools for greater efficiency. HuggingFace is a website that provides numerous tools and resources for developers and researchers. It also has a wide variety of specialized and high-accuracy models. HuggingGPT uses these models for sophisticated AI tasks in different domains and modalities thereby achieving impressive results. It has similar multimodal capabilities to OPenAI GPT-4 when it comes to text and images. But, it also connected you to the Internet and you can provide an external web link to ask questions about it.\n",
            "Suppose you want the model to generate an audio reading of the text written on an image. HuggingGPT will perform this task serially using the best-suited models. Firstly, it will generate the image from text and use its result for audio generation. You can check the response details in the image below. Simply Amazing!\n",
            "Qualitative analysis of multi-model cooperation on video and audio modalities (Source)\n",
            "How Does HuggingGPT Work?\n",
            "Image by Author\n",
            "HuggingGPT is a collaborative system that uses LLMs as an interface to send user requests to expert models. The complete process starting from the user prompt to the model till receiving the response can be broken down into the following discrete steps:\n",
            "1. Task Planning\n",
            "In this stage, HuggingGPT makes use of ChatGPT to understand the user prompt and then breaks down the query into small actionable tasks. It also determines the dependencies of these tasks and defines their execution sequence. HuggingGPT has four slots for task parsing i.e. task type, task ID, task dependencies, and task arguments. Chat logs between the HuggingGPT and the user are recorded and displayed on the screen that shows the history of the resources.\n",
            "2. Model Selection\n",
            "Based on the user context and the available models, HuggingGPT uses an in-context task-model assignment mechanism to select the most appropriate model for a particular task. According to this mechanism, the selection of a model is considered a single-choice problem and it initially filters out the model based on the type of the task. After that, the models are ranked based on the number of downloads as it is considered a reliable measure that reflects the quality of the model. “Top-K” models are selected based on this ranking. Here K is just a constant that reflects the number of models, for example, if it is set to 3 then it will select 3 models with the highest number of downloads.\n",
            "3. Task Execution\n",
            "Here the task is assigned to a specific model, it performs the inference on it and returns the result. To enhance the efficiency of this process, HuggingGPT can run different models at the same time as long as they don’t need the same resources. For example, if I give a prompt to generate pictures of cats and dogs then separate models can run in parallel to execute this task. However, sometimes models may need the same resources which is why HuggingGPT maintains an <resource> attribute to keep the track of the resources. It ensures that the resources are being used effectively.\n",
            "4. Response Generation\n",
            "The final step involves generating the response to the user. Firstly, it integrates all the information from the previous stages and the inference results. The information is presented in a structured format. For example, if the prompt was to detect the number of lions in an image, it will draw the appropriate bounding boxes with detection probabilities. The LLM (ChatGPT) then uses this format and presents it in human-friendly language.\n",
            "Setting Up HuggingGPT\n",
            "HuggingGPT is built on top of Hugging Face's state-of-the-art GPT-3.5 architecture, which is a deep neural network model that can generate natural language text. Here is how you can set it up on your local computer:\n",
            "System Requirements\n",
            "The default configuration requires Ubuntu 16.04 LTS, VRAM of at least 24GB, RAM of at least 12GB (minimal), 16GB (standard), or 80GB (full), and disk space of at least 284 GB. Additionally, you'll need 42GB of space for damo-vilab/text-to-video-ms-1.7b, 126GB for ControlNet, 66GB for stable-diffusion-v1-5, and 50GB for other resources. For the \"lite\" configuration, you'll only need Ubuntu 16.04 LTS.\n",
            "Steps to Get Started\n",
            "First, replace the OpenAI Key and the Hugging Face Token in the server/configs/config.default.yaml file with your keys. Alternatively, you can put them in the environment variables OPENAI_API_KEY and HUGGINGFACE_ACCESS_TOKEN, respectively\n",
            "Run the following commands:\n",
            "For Server:\n",
            "- Set up the Python environment and install the required dependencies.\n",
            "# setup env cd server conda create -n jarvis python=3.8 conda activate jarvis conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia pip install -r requirements.txt\n",
            "- Download the required models.\n",
            "# download models. Make sure that `git-lfs` is installed. cd models bash download.sh # required when `inference_mode` is `local` or `hybrid`.\n",
            "- Run the server\n",
            "# run server cd .. python models_server.py --config configs/config.default.yaml # required when `inference_mode` is `local` or `hybrid` python awesome_chat.py --config configs/config.default.yaml --mode server # for text-davinci-003\n",
            "Now you can access Jarvis' services by sending HTTP requests to the Web API endpoints. Send a request to :\n",
            "- /hugginggpt endpoint using the POST method to access the full service.\n",
            "- /tasks endpoint using the POST method to access intermediate results for Stage #1\n",
            "- /results endpoint using the POST method to access intermediate results for Stages #1-3.\n",
            "The requests should be in JSON format and should include a list of messages that represent the user's inputs.\n",
            "For Web:\n",
            "- Install node js and npm on your machine after starting your application awesome_chat.py in server mode.\n",
            "- Navigate to the web directory and install the following dependencies\n",
            "cd web npm install npm run dev\n",
            "- Set http://{LAN_IP_of_the_server}:{port}/ to HUGGINGGPT_BASE_URL of web/src/config/index.ts, in case you are running the web client on another machine.\n",
            "- If you want to use the video generation feature, compile ffmpeg manually with H.264.\n",
            "# Optional: Install ffmpeg # This command needs to be executed without errors. LD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/ffmpeg -i input.mp4 -vcodec libx264 output.mp4\n",
            "- Double-click on the setting icon to switch back to ChatGPT.\n",
            "For CLI:\n",
            "Setting up Jarvis using CLI is quite simple. Just run the command mentioned below:\n",
            "cd server python awesome_chat.py --config configs/config.default.yaml --mode cli\n",
            "For Gradio:\n",
            "Gradio demo is also being hosted on Hugging Face Space. You can experiment with it after entering the OPENAI_API_KEY and HUGGINGFACE_ACCESS_TOKEN.\n",
            "To run it locally:\n",
            "- Install the required dependencies, clone the project repository from the Hugging Face Space, and navigate to the project directory\n",
            "- Start the model server followed by the Gradio demo using:\n",
            "python models_server.py --config configs/config.gradio.yaml python run_gradio_demo.py --config configs/config.gradio.yaml\n",
            "- Access the demo in your browser at http://localhost:7860 and test by entering various inputs\n",
            "- Optionally, you can also run the demo as a Docker image by running the following command:\n",
            "docker run -it -p 7860:7860 --platform=linux/amd64 registry.hf.space/microsoft-hugginggpt:latest python app.py\n",
            "Note: In case of any issue please refer to the official Github Repo.\n",
            "Final Thoughts\n",
            "HuggingGPT also has certain limitations that I want to highlight here. For instance, the efficiency of the system is a major bottleneck and during all the stages mentioned earlier, HuggingGPT requires multiple interactions with LLMs. These interactions can lead to degraded user experience and increased latency. Similarly, the maximum context length is also limited by the number of allowed tokens. Another problem is the System's reliability, as the LLMs may misinterpret the prompt and generate a wrong sequence of tasks which in turn affects the whole process. Nonetheless, it has significant potential to solve complex AI tasks and is an excellent advancement toward AGI. Let's see in which direction this research leads us too. That’s a wrap, feel free to express your views in the comment section below.\n",
            "Kanwal Mehreen is an aspiring software developer with a keen interest in data science and applications of AI in medicine. Kanwal was selected as the Google Generation Scholar 2022 for the APAC region. Kanwal loves to share technical knowledge by writing articles on trending topics, and is passionate about improving the representation of women in tech industry.\n",
            "⏳ Summarizing the text.\n",
            "✅ Writing summaries to file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5: Download the resulting .docx. Finished! \n",
        "from google.colab import files\n",
        "import datetime\n",
        "\n",
        "date_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "output_filename = f\"summaries-{date_str}.docx\"\n",
        "files.download(f\"/content/{output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "id": "bw8fRT9LElxY",
        "outputId": "c0f7e6f7-e01e-4b82-979a-55def68c1d74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9564a9cc-a199-4c38-a49e-5229908a7868\", \"summaries-2023-05-06.docx\", 38068)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}